{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텍스트 전처리(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnBfQcNtg6RQOYAmNT+gAr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1UmHNIoX6dV"
      },
      "source": [
        "## 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\r\n",
        "---\r\n",
        "\r\n",
        "이번 챕터에서는 정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법인 표제어 추출(lemmatization)과 어간 추출(stemming)의 개념에 대해서 알아봅니다. 또한 이 둘의 결과가 어떻게 다른지 이해합니다.\r\n",
        "\r\n",
        "이 두 작업이 갖고 있는 의미는 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것입니다. 이러한 방법들은 단어의 빈도수를 기반으로 문제를 풀고자 하는 BoW(Bag of Words) 표현을 사용하는 자연어 처리 문제에서 주로 사용됩니다. BoW 표현은 뒤의 4챕터에서 배우게 됩니다. 자연어 처리에서 전처리, 더 정확히는 정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPYRHmfqX-Z1"
      },
      "source": [
        "### 1. 표제어 추출(Lemmatization)\r\n",
        "---\r\n",
        "\r\n",
        "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미를 갖습니다. 표제어 추출은 단어들로부터 표제어를 찾아가는 과정입니다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이 때, 이 단어들의 표제어는 be라고 합니다.\r\n",
        "\r\n",
        "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것입니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻합니다.\r\n",
        "\r\n",
        "형태소는 두 가지 종류가 있습니다. 각각 어간(stem)과 접사(affix)입니다.<br>\r\n",
        "**1) 어간(stem)**: 단어의 의미를 담고 있는 단어의 핵심 부분.<br>\r\n",
        "**2) 접사(affix)**: 단어에 추가적인 의미를 주는 부분\r\n",
        "\r\n",
        "형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말합니다. 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리합니다. 꼭 두 가지로 분리되지 않는 경우도 있습니다. 단어 fox는 형태학적 파싱을 한다고 하더라도 더 이상 분리할 수 없습니다. fox는 독립적인 형태소이기 때문입니다. 이와 유사하게 cat 또한 더 이상 분리되지 않습니다.\r\n",
        "\r\n",
        "NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원합니다. 이를 통해 표제어 추출을 실습해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiybLIe0XtFG",
        "outputId": "a71e0830-3b6f-403a-c507-3ce82130b7d0"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHbA-0YlYBIM",
        "outputId": "ccadcc78-17d7-4e22-9443-22d59bfa8892"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "n=WordNetLemmatizer()\r\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\r\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51jrj5yVYFNM"
      },
      "source": [
        "잠시 후 어간 추출(stemming)에 대해서 배우고, 같은 입력에 대한 결과를 비교해보시면 알겠지만 표제어 추출은 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다. 하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. 이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.\r\n",
        "\r\n",
        "WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다. 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y7A9vs5WYET0",
        "outputId": "b44973ef-cd25-4ee1-dc41-9c2956d9eb93"
      },
      "source": [
        "n.lemmatize('dies', 'v')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "icPPHLcJYKOm",
        "outputId": "bd4c35c6-6855-4192-89aa-30e81b28e6d8"
      },
      "source": [
        "n.lemmatize('watched', 'v')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y9yEfTkCYLak",
        "outputId": "5f000fe8-dc1c-4074-8ca5-3b5656a149d7"
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODHnp1AAYNRD"
      },
      "source": [
        "어간 추출에 대해서 언급하기에 앞서, 표제어 추출과 어간 추출의 차이에 대해 미리 언급하고자 합니다. 표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. (POS 태그를 보존한다고도 말할 수 있습니다.)\r\n",
        "\r\n",
        "하지만, 어간 추출을 수행한 결과는 품사 정보가 보존되지 않습니다. (다시 말해 POS 태그를 고려하지 않습니다.) 더 정확히는, 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd8HXwPAYPcu"
      },
      "source": [
        "### 2. 어간 추출(Stemming)\r\n",
        "---\r\n",
        "\r\n",
        "어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다.\r\n",
        "\r\n",
        "설명이 어렵게 느껴질 수 있는데, 예제를 보면 쉽게 이해할 수 있습니다. 어간 추출 알고리즘 중 하나인 포터 알고리즘(Porter Algorithm)에 아래의 Text를 입력으로 넣는다고 해봅시다.\r\n",
        "\r\n",
        "**input**: This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiebkDwVYMj9",
        "outputId": "3c8ce824-98ce-4c9d-e933-87d96b8a307a"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "s = PorterStemmer()\r\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\r\n",
        "words=word_tokenize(text)\r\n",
        "print(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7LOjOcHYTXU",
        "outputId": "a2d5e3ea-abf2-462f-c1c2-606614c7fb9c"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgrUkH7TYeKJ"
      },
      "source": [
        "위의 알고리즘의 결과에는 사전에 없는 단어들도 포함되어 있습니다. 위의 어간 추출은 단순 규칙에 기반하여 이루어지기 때문입니다.\r\n",
        "\r\n",
        "가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.\r\n",
        "\r\n",
        "ALIZE → AL<br>\r\n",
        "ANCE → 제거<br>\r\n",
        "ICAL → IC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwCrEF4lYdHp",
        "outputId": "4e23a3c5-2694-4133-fcaa-54b8164169de"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']\r\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4rnoOPbYiCr"
      },
      "source": [
        "위의 규칙에 따라서 다음과 같은 결과가 나옵니다.\r\n",
        "\r\n",
        "formalize → formal<br>\r\n",
        "allowance → allow<br>\r\n",
        "electricical → electric\r\n",
        "\r\n",
        "**※ Poter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인할 수 있다.**\r\n",
        "\r\n",
        "어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다. NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다. 이번에는 포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때, 이 둘의 결과를 비교해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWrNLqOyYhcB",
        "outputId": "677b8859-8935-4f34-dc88-1aebedabe81e"
      },
      "source": [
        "from nltk.stem import PorterStemmer\r\n",
        "s=PorterStemmer()\r\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\r\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xq4iLjvYmcj",
        "outputId": "dc224add-6fd6-438c-9d87-5060031689a6"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\r\n",
        "l=LancasterStemmer()\r\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\r\n",
        "print([l.stem(w) for w in words])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZIMEulgYoiN"
      },
      "source": [
        "동일한 단어들의 나열에 대해서 두 스태머는 전혀 다른 결과를 보여줍니다. 두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문입니다. 그렇기 때문에 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용하여야 합니다.\r\n",
        "\r\n",
        "이런 규칙에 기반한 알고리즘은 종종 제대로 된 일반화를 수행하지 못 할 수 있습니다. 어간 추출을 하고나서 일반화가 지나치게 되거나, 또는 덜 되거나 하는 경우입니다. 예를 들어 포터 알고리즘에서 organization을 어간 추출했을 때의 결과를 봅시다.\r\n",
        "\r\n",
        "organization → organ\r\n",
        "\r\n",
        "organization과 organ은 완전히 다른 단어 임에도 organization에 대해서 어간 추출을 했더니 organ이라는 단어가 나왔습니다. organ에 대해서 어간 추출을 한다고 하더라도 결과는 역시 organ이 되기 때문에, 두 단어에 대해서 어간 추출을 한다면 동일한 어간을 갖게 됩니다. 이는 어간 추출의 목적에는 맞지 않습니다.\r\n",
        "\r\n",
        "마지막으로, 같은 단어에 대해서 표제어 추출과 어간 추출을 각각 수행했을 때, 결과에서 어떤 차이가 있는지 간단한 예를 보겠습니다.\r\n",
        "\r\n",
        "**Stemming**<br>\r\n",
        "am → am<br>\r\n",
        "the going → the go<br>\r\n",
        "having → hav\r\n",
        "\r\n",
        "**Lemmatization**<br>\r\n",
        "am → be<br>\r\n",
        "the going → the going<br>\r\n",
        "having → have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1CLM09YtPP"
      },
      "source": [
        "### 3. 한국어에서의 어간 추출\r\n",
        "---\r\n",
        "\r\n",
        "한국어에 대해서는 별도 실습은 진행하지 않지만, 한국어의 어간에 대해서 간략히 설명합니다. 한국어는 아래의 표와 같이 5언 9품사의 구조를 가지고 있습니다.\r\n",
        "\r\n",
        "|**언**|**품사**|\r\n",
        "|--|-------|\r\n",
        "|체언|명사, 대명사, 수사|\r\n",
        "|수식언|관형사, 부사|\r\n",
        "|관계언|조사|\r\n",
        "|독립언|감탄사|\r\n",
        "|**용언**|**동사, 형용사**|\r\n",
        "\r\n",
        "이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됩니다. 앞으로 용언이라고 언급하는 부분은 전부 동사와 형용사를 포함하여 언급하는 개념입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dqeDVP6Yw4P"
      },
      "source": [
        "#### (1) 활용(conjugation)\r\n",
        "\r\n",
        "활용(conjugation)은 한국어에서만 가지는 특징이 아니라, 인도유럽어(indo-european language)에서도 주로 볼 수 있는 언어적 특징 중 하나를 말하는 통칭적인 개념입니다. 다만, 여기서는 한국어에 한정하여 설명합니다.\r\n",
        "\r\n",
        "활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다.\r\n",
        "\r\n",
        "**어간(stem)**: 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\r\n",
        "\r\n",
        "**어미(ending)**: 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\r\n",
        "\r\n",
        "활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL7dhyLPYzmk"
      },
      "source": [
        "#### (2) 규칙 활용\r\n",
        "\r\n",
        "규칙 활용은 어간이 어미를 취할 때, 어간의 모습이 일정합니다. 아래의 예제는 어간과 어미가 합쳐질 때, 어간의 형태가 바뀌지 않음을 보여줍니다\r\n",
        "\r\n",
        "```\r\n",
        "잡/어간 + 다/어미\r\n",
        "```\r\n",
        "\r\n",
        "이 경우에는 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRQt4utdY2ad"
      },
      "source": [
        "#### (3) 불규칙 활용\r\n",
        "\r\n",
        "불규칙 활용은 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우를 말합니다.\r\n",
        "\r\n",
        "예를 들어 ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’와 같이 어간의 형식이 달라지는 일이 있거나 ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’와 같이 일반적인 어미가 아닌 특수한 어미를 취하는 경우 불규칙활용을 하는 예에 속합니다.\r\n",
        "\r\n",
        "이 경우에는 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요로 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCvhuOJCZN1K"
      },
      "source": [
        "## 04)불용어\r\n",
        "\r\n",
        "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, **I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우**가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.\r\n",
        "\r\n",
        "물론 불용어는 개발자가 직접 정의할 수도 있습니다. 이번 챕터에서는 영어 문장에서 NLTK가 정의한 영어 불용어를 제거하는 실습을 하고, 한국어 문장에서 직접 정의한 불용어를 제거해보겠습니다.\r\n",
        "\r\n",
        "NLTK 실습에서는 1챕터에서 언급했듯이 NLTK Data가 필요합니다. 만약, 데이터가 없다는 에러가 발생 시에는 nltk.download(필요한 데이터)라는 커맨드를 통해 다운로드 할 수 있습니다. 해당 커맨드 또한 에러가 발생할 경우 1챕터의 NLTK Data 가이드를 참고 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJx5HfXbZRpP"
      },
      "source": [
        "### 1. NLTK에서 불용어 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUrq98tNYnts",
        "outputId": "87f9bbc0-fa50-4b61-e11e-5773b90143f1"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stopwords.words('english')[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAPv20JZVCz"
      },
      "source": [
        "stopwords.words(\"english\")는 NLTK가 정의한 영어 불용어 리스트를 리턴합니다. 출력 결과가 100개 이상이기 때문에 여기서는 간단히 10개만 확인해보았습니다. 'i', 'me', 'my'와 같은 단어들을 NLTK에서 불용어로 정의하고 있음을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC6zIHJkZZWL"
      },
      "source": [
        "### 2. NLTK를 통해서 불용어 제거하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I66hBrdYZT6I",
        "outputId": "be94b06a-80ab-4b27-cc42-de51eb310de0"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "example = \"Family is an important thing. It's everything.\"\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "word_tokens = word_tokenize(example)\r\n",
        "# print(word_tokens)\r\n",
        "\r\n",
        "result = []\r\n",
        "for w in word_tokens:\r\n",
        "    if w not in stop_words:\r\n",
        "        result.append(w)\r\n",
        "\r\n",
        "print(word_tokens)\r\n",
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Family', 'is', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3I22GOZeKp"
      },
      "source": [
        "위 코드는 \"Family is not an important thing. It's everything.\"라는 임의의 문장을 정의하고, NLTK가 정의하고 있는 불용어를 제외한 결과를 출력하고 있습니다. 'is', 'not', 'an'과 같은 단어들이 문장에서 제거되었음을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXDL0GPUZj5r"
      },
      "source": [
        "### 3. 한국어에서 불용어 제거하기\r\n",
        "\r\n",
        "한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있습니다. 하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도 합니다. 결국에는 사용자가 직접 불용어 사전을 만들게 되는 경우가 많습니다. 이번에는 직접 불용어를 정의해보고, 주어진 문장으로부터 직접 정의한 불용어 사전을 참고로 불용어를 제거해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdVtgfOxZcl3",
        "outputId": "f720a400-3293-4d7e-8d84-f17ea605ed30"
      },
      "source": [
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "\r\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\r\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 예컨대 이럴정도로 하면 아니거든\"\r\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\r\n",
        "stop_words=stop_words.split(' ')\r\n",
        "word_tokens = word_tokenize(example)\r\n",
        "\r\n",
        "result = []\r\n",
        "for w in word_tokens:\r\n",
        "    if w not in stop_words:\r\n",
        "        result.append(w)\r\n",
        "# 위의 4줄은 아래의 한 줄로 대체 가능\r\n",
        "# result=[word for word in word_tokens if not word in stop_words]\r\n",
        "\r\n",
        "print(word_tokens)\r\n",
        "print(result)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA4T6wcVZoF_"
      },
      "source": [
        "아래의 링크는 보편적으로 선택할 수 있는 한국어 불용어 리스트를 보여줍니다. (여전히 절대적인 기준은 아닙니다.)\r\n",
        "\r\n",
        "링크: [한국어 불용어 리스트](https://www.ranks.nl/stopwords/korean)\r\n",
        "\r\n",
        "한국어 불용어를 제거하는 더 좋은 방법은 코드 내에서 직접 정의하지 않고 txt 파일이나 csv 파일로 수많은 불용어를 정리해놓고, 이를 불러와서 사용하는 방법입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuW9grOlZ-nV"
      },
      "source": [
        "https://wikidocs.net/21690 자료를 이용해 학습"
      ]
    }
  ]
}